{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Philip Walsh\n",
    "# philipwalsh.ds@gmail.com\n",
    "# 2019-12-20\n",
    "# Starter notebook can be used as a jumping off point to the challenge\n",
    "# Kaggle - House Prices: Advanced Regression Techniques\n",
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_train_data  : training.csv (provided, contains SalePrice)\n",
    "# X_train              : 80% of the training data - for fitting the model\n",
    "# X_hold_out           : 20% of the training data - for evaluating the model on unseen data\n",
    "# submissions_data     : test.csv (provided, does not contain SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_intermediate_work = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_fit(X, Y):\n",
    "    # to plot the line of best fit\n",
    "    # https://stackoverflow.com/questions/22239691/code-for-best-fit-straight-line-of-a-scatter-plot-in-python\n",
    "    # aziz alto\n",
    "    xbar = sum(X)/len(X)\n",
    "    ybar = sum(Y)/len(Y)\n",
    "    n = len(X) # or len(Y)\n",
    "\n",
    "    numer = sum([xi*yi for xi,yi in zip(X, Y)]) - n * xbar * ybar\n",
    "    denum = sum([xi**2 for xi in X]) - n * xbar**2\n",
    "\n",
    "    b = numer / denum\n",
    "    a = ybar - b * xbar\n",
    "\n",
    "    print('best fit line:\\ny = {:.2f} + {:.2f}x'.format(a, b))\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##usage\n",
    "##best fit line:\n",
    "##y = 0.80 + 0.92x\n",
    "#\n",
    "#a, b = best_fit(X, Y)\n",
    "#\n",
    "## plot points and fit line\n",
    "#plt.scatter(X, Y)\n",
    "#yfit = [a + b * xi for xi in X]\n",
    "#plt.plot(X, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_data = pd.read_csv('excluded/train.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePrice is what we are being asked to predict\n",
    "# so lets havea quick look \n",
    "plt.hist(complete_train_data['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(complete_train_data['SalePrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_data['LotFrontage'].fillna(complete_train_data['LotFrontage'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot of LotFrontage vs SalePrice\n",
    "# add in line of best fit as well\n",
    "X = complete_train_data['LotFrontage']\n",
    "Y = complete_train_data['SalePrice']\n",
    "a, b = best_fit(X, Y)\n",
    "\n",
    "# plot points and fit line\n",
    "plt.scatter(X, Y)\n",
    "plt.xlabel('LotFrontage')\n",
    "plt.ylabel('SalePrice')\n",
    "yfit = [a + b * xi for xi in X]\n",
    "plt.plot(X, yfit, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(complete_train_data['LotFrontage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_train_data['GrLivArea']\n",
    "Y = complete_train_data['SalePrice']\n",
    "a, b = best_fit(X, Y)\n",
    "#best fit line:\n",
    "#y = 0.80 + 0.92x\n",
    "\n",
    "# plot points and fit line\n",
    "plt.xlabel('GrLivArea')\n",
    "plt.ylabel('SalePrice')\n",
    "plt.scatter(X, Y)\n",
    "yfit = [a + b * xi for xi in X]\n",
    "plt.plot(X, yfit, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('GrvLivArea')\n",
    "plt.boxplot(complete_train_data['GrLivArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## pull in the \"test.csv\" so we can do some feature engineering and data cleaning\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in the submission data, aka the test data\n",
    "# we will merge the train/test data into one large set, for cleaning purposes\n",
    "submission_data = pd.read_csv('excluded/test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## steps will be\n",
    "##  Split up the train data into train/hold_out\n",
    "##  Tag all 3 sets with a variable that identifies their purpose\n",
    "##  Combine all 3 sets train+hold_out+submission(aka test)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up bins for the stratified split\n",
    "complete_train_data['living_area_cat'] = pd.cut(\n",
    "    complete_train_data['GrLivArea'], \n",
    "    bins=[0, 500, 1000, 1500, 2000, 2500, np.inf], \n",
    "    labels=[1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_train_data[['SalePrice','GrLivArea','living_area_cat']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=.20, random_state=9261774)\n",
    "for train_index, hold_out_index in split.split(complete_train_data, complete_train_data['living_area_cat']):\n",
    "    X_train = complete_train_data.iloc[train_index].copy() # this is the training data\n",
    "    X_hold_out = complete_train_data.iloc[hold_out_index].copy()   # this is the hold out, the protion of the training i will use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next we save the SalePrice, remove it from the train/hold_out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the label/expected/y variable\n",
    "y_train = X_train['SalePrice']\n",
    "y_hold_out = X_hold_out['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop SalePrice from the x vars\n",
    "X_train.drop('SalePrice', axis=1, inplace=True)\n",
    "X_hold_out.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done with the living_area_cat\n",
    "X_train.drop('living_area_cat', axis=1, inplace=True)\n",
    "X_hold_out.drop('living_area_cat', axis=1, inplace=True)\n",
    "complete_train_data.drop('living_area_cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new TRAIN field to keep the different data sets from getting mixed up\n",
    "X_train['TRAIN']=1          # 1 indicates its from the training data\n",
    "X_hold_out['TRAIN']=0       # 0 indicates its hold-out\n",
    "submission_data['TRAIN']=-1 #-1 for the submissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hold_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=pd.concat([X_train, X_hold_out, submission_data],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for missing data\n",
    "#pd.set_option('display.max_rows', 10)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "combined[['OverallQual','OverallCond','LotFrontage']].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanData(df_in):\n",
    "    # lotFrontage was identified as a column with missing data\n",
    "    # let's do a quick fill\n",
    "    df_clean = df_in.copy()\n",
    "    temp_mean = df_clean['LotFrontage'].mean()\n",
    "    temp_mean\n",
    "    df_clean['LotFrontage'].fillna(temp_mean, inplace=True)\n",
    "    \n",
    "    # multiply the condition and quality to create a new variable\n",
    "    df_clean['OverallQualCond'] = (df_clean['OverallQual']+1) * (df_clean['OverallCond']+1)\n",
    "    df_clean.drop('OverallQual',axis=1, inplace=True)\n",
    "    df_clean.drop('OverallCond',axis=1, inplace=True)\n",
    "    \n",
    "    #create a centered LotFrontage\n",
    "    df_clean['LotFrontageCtr']=df_clean['LotFrontage']-temp_mean\n",
    "    \n",
    "    #create a normalized LotFrontage\n",
    "    df_clean['LotFrontageNorm']=(df_clean['LotFrontage']-df_clean['LotFrontage'].min())/(df_clean['LotFrontage'].max()-df_clean['LotFrontage'].min())\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = CleanData(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## data is clean, features engineered\n",
    "## time to ripe the sets apart again, and do some modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - use to fit models\n",
    "X_train = combined[combined['TRAIN']==1].copy()\n",
    "X_train.drop('TRAIN', axis=1, inplace=True)\n",
    "\n",
    "# sendtofile(excluded_dir, 'X_train.csv', X_train, verbose=True)\n",
    "\n",
    "\n",
    "# hold_out - use to evaluate model performance\n",
    "X_hold_out = combined[combined['TRAIN']==0].copy()\n",
    "X_hold_out.drop('TRAIN', axis=1, inplace=True)\n",
    "\n",
    "# submission - use to submit the final answer for the kaggle cometition\n",
    "X_submission = combined[combined['TRAIN']==-1].copy()\n",
    "X_submission.drop('TRAIN', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hold_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cols = ['LotFrontage','GrLivArea']  #0.5049479993923547\n",
    "#train_cols = ['LotFrontageNorm','GrLivArea']\n",
    "train_cols = ['LotFrontage','GrLivArea','OverallQualCond']  #0.5972533775871418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit a linear regression model\n",
    "model_lr = LinearRegression(normalize=False)\n",
    "model_lr.fit(X_train[train_cols], np.log(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## score the model\n",
    "train_score_lm=model_lr.score(X_train[train_cols], np.log(y_train))\n",
    "print('lm training score     : ', train_score_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_score_lm=model_lr.score(X_hold_out[train_cols], np.log(y_hold_out))\n",
    "print('lm test score         : ', hold_out_score_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = the coefficient of determination R^2 of the prediction \n",
    "#The coefficient R^2 is defined as (1 - u/v), \n",
    "#  where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() \n",
    "#  and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). \n",
    "#  The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training data through the predict to get the residuals\n",
    "predicted = np.exp(pd.DataFrame(model_lr.predict(X_train[train_cols])))\n",
    "expected = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_intermediate_work:\n",
    "    predicted.to_csv('excluded/log-test-predicted-train-lr.csv',index=False)\n",
    "    pd.DataFrame(expected).to_csv('excluded/log-test-expected-train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted vs the expected.  \n",
    "# how well did the model perform\n",
    "plt.plot([0,800000],[0,800000], color='red')\n",
    "plt.title('Visualize the predicictions performance')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('expected/observed')\n",
    "plt.scatter(predicted, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.array(expected.values.ravel()) - np.array(predicted.values.ravel())\n",
    "plt.title('residuals')\n",
    "plt.xlabel('expected/observed')\n",
    "plt.ylabel('residuals')\n",
    "plt.plot([0,800000],[0,0], color='red')\n",
    "plt.scatter(expected, residuals)\n",
    "# a good residual plot does not have a pattern\n",
    "# a pattern in the plot means we missed in capturing the data thats driving sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a preditction on the hold out data\n",
    "predicted = np.exp(pd.DataFrame(model_lr.predict(X_hold_out[train_cols])))\n",
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = y_hold_out\n",
    "expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_intermediate_work:\n",
    "    predicted.to_csv('excluded/log-test-predicted-hold-out-lr.csv',index=False)\n",
    "    pd.DataFrame(expected).to_csv('excluded/log-test-expected-hold-out.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted vs the expected.  \n",
    "# how well did the model perform\n",
    "plt.plot([0,800000],[0,800000], color='red')\n",
    "plt.title('Visualize the predicictions performance')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('expected/observed')\n",
    "plt.scatter(predicted, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(expected.values.ravel()) - np.array(predicted.values.ravel())\n",
    "plt.title('errors')\n",
    "plt.xlabel('expected/observed')\n",
    "plt.ylabel('errors')\n",
    "plt.plot([0,800000],[0,0], color='red')\n",
    "plt.scatter(expected, errors)\n",
    "# a good residual plot does not have a pattern\n",
    "# a pattern in the plot means we missed in capturing the data thats driving sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest\n",
    "model_rf = RandomForestRegressor(random_state=9261774, n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', max_depth=None, bootstrap=False)\n",
    "model_rf.fit(X_train[train_cols], y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_rf=model_rf.score(X_train[train_cols], y_train)\n",
    "train_score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hold_out[train_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_score_rf=model_rf.score(X_hold_out[train_cols], y_hold_out)\n",
    "hold_out_score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training data through the predict to get the residuals\n",
    "predicted = pd.DataFrame(model_rf.predict(X_train[train_cols]))\n",
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = y_train\n",
    "expected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_intermediate_work:\n",
    "    predicted.to_csv('excluded/log-test-predicted-train-rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted vs the expected.  \n",
    "# how well did the model perform\n",
    "plt.plot([0,800000],[0,800000], color='red')\n",
    "plt.title('Visualize the predicictions performance')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('expected/observed')\n",
    "plt.scatter(predicted, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.array(expected.values.ravel()) - np.array(predicted.values.ravel())\n",
    "plt.title('residuals')\n",
    "plt.xlabel('expected/observed')\n",
    "plt.ylabel('residuals')\n",
    "plt.plot([0,800000],[0,0], color='red')\n",
    "plt.scatter(expected, residuals)\n",
    "# a good residual plot does not have a pattern\n",
    "# a pattern in the plot means we missed in capturing the data thats driving sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a preditction on the hold out data\n",
    "predicted = pd.DataFrame(model_rf.predict(X_hold_out[train_cols]))\n",
    "# put \n",
    "expected = y_hold_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted vs the expected.  \n",
    "# how well did the model perform\n",
    "plt.plot([0,800000],[0,800000], color='red')\n",
    "plt.title('Visualize the predicictions performance')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('expected/observed')\n",
    "plt.scatter(predicted, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array(expected.values.ravel()) - np.array(predicted.values.ravel())\n",
    "plt.title('errors')\n",
    "plt.xlabel('expected/observed')\n",
    "plt.ylabel('errors')\n",
    "plt.plot([0,800000],[0,0], color='red')\n",
    "plt.scatter(expected, errors)\n",
    "# a good residual plot does not have a pattern\n",
    "# a pattern in the plot means we missed in capturing the data thats driving sale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### proceed past this point when ready to make a prediction\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission[train_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the vars we are about to use in the predict cal, are there any NaN(s)?\n",
    "X_submission[train_cols].isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lr predict \n",
    "lr_submission_df = X_submission[['Id']]\n",
    "lr_prediction_y=model_lr.predict(X_submission[train_cols])\n",
    "# tack the saved predictions onto the preds into a data frame\n",
    "lr_pred_df=pd.DataFrame(lr_prediction_y, columns=['SalePrice'])\n",
    "lr_submission_df = pd.concat([lr_submission_df,lr_pred_df], axis='columns', sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save submission file\n",
    "lr_submission_df.to_csv('excluded/_log-test-submission-lr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rf predict \n",
    "rf_submission_df = X_submission[['Id']]\n",
    "rf_prediction_y=model_rf.predict(X_submission[train_cols])\n",
    "# tack the saved predictions onto the preds into a data frame\n",
    "rf_pred_df=pd.DataFrame(rf_prediction_y, columns=['SalePrice'])\n",
    "rf_submission_df = pd.concat([rf_submission_df,rf_pred_df], axis='columns', sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submission_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save submission file\n",
    "rf_submission_df.to_csv('excluded/_log-test-submission-rf.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rf_submission_df = pd.concat([lr_submission_df,rf_pred_df], axis='columns', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rf_submission_df.columns=['Id', 'SalePrice_LR','SalePrice_RF']\n",
    "\n",
    "lr_rf_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rf_submission_df['SalePrice'] = (lr_rf_submission_df['SalePrice_LR'] * 0.50) + (lr_rf_submission_df['SalePrice_RF'] * 0.50)\n",
    "lr_rf_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rf_submission_df.drop('SalePrice_LR', axis=1, inplace=True)\n",
    "lr_rf_submission_df.drop('SalePrice_RF', axis=1, inplace=True)\n",
    "lr_rf_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rf_submission_df.to_csv('excluded/_log-test-submission-lr-rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above code received a kaggle score of \n",
    "#\n",
    "#   0.28700  train_cols = ['LotFrontage','GrLivArea'] - lr\n",
    "#\n",
    "#   0.24117  train_cols = ['LotFrontage','GrLivArea','OverallQualCond'] - lr\n",
    "#   0.22596  train_cols = ['LotFrontage','GrLivArea','OverallQualCond'] - rf\n",
    "#   0.21537  train_cols = ['LotFrontage','GrLivArea','OverallQualCond'] - ensemble lr+rf\n",
    "#\n",
    "\n",
    "\n",
    "# More data analysis needed\n",
    "# Practically every independent variable in this dataset is useful\n",
    "# there are a few variablex that have excessive NaN(s), probably best to drop the variables\n",
    "# Create combined columns, ex total bathroom count, total overall condition score\n",
    "# Separate the \"categorical\" type columns into numeric categoricals and one hot encoded variables\n",
    "# create additional models and ensemble them together\n",
    "#    model_rf = RandomForestRegressor(random_state=9261774, n_estimators=400, ...\n",
    "#    model_gb = GradientBoostingRegressor(random_state=9261774,learning_rate=0.025,...\n",
    "#    ...\n",
    "#    submission_lr_rf_gb['SalePrice']=(\n",
    "#        submission_lr_rf_gb['SalePrice_LR'] * 0.20 +\n",
    "#        submission_lr_rf_gb['SalePrice_RF'] * 0.30 +\n",
    "#        submission_lr_rf_gb['SalePrice_GB'] * 0.50 )\n",
    "# \n",
    "# my best score, with a few days of data cleaning/feature engineering\n",
    "#\n",
    "# 0.12593\n",
    "# 1744/5749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37_1)",
   "language": "python",
   "name": "py37_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
